{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# import torch\n",
    "# from torchvision import models, transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# from keras.preprocessing.image import load_img \n",
    "# from keras.preprocessing.image import img_to_array \n",
    "# from keras.applications.vgg16 import preprocess_input \n",
    "# from keras.applications.vgg16 import VGG16 \n",
    "# from keras.models import Model\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to be changed into downloading form .zip or somewhere else (.zip of compressed rotated set of images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of files to process: 36983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['img-fm1000215.jpg',\n",
       " 'img-fm1001186.jpg',\n",
       " 'img-fm1001570.jpg',\n",
       " 'img-fm1022730.jpg',\n",
       " 'img-fm1022968.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = 'images/images'\n",
    "file_names = os.listdir(folder_path)\n",
    "file_names = [f for f in file_names if os.path.isfile(os.path.join(folder_path, f))]\n",
    "print(f'Amount of files to process: {len(file_names)}')\n",
    "file_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG + ORB + KMeans\n",
    "This implimentation works with grayscaled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For single image and its key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "image_path = 'images/images/' + file_names[98]\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create(nfeatures=1000)  # Limit to 500 keypoints\n",
    "\n",
    "# Detect keypoints and descriptors\n",
    "keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "\n",
    "# # Visualize keypoints\n",
    "# image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0))\n",
    "# plt.imshow(image_with_keypoints, cmap='gray')\n",
    "# plt.title('ORB Keypoints')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000 files - 2 minutes\n",
    "# Initialize ORB detector\n",
    "orb = cv2.ORB_create(nfeatures=1000)  # Limit to 500 keypoints per image\n",
    "\n",
    "# Function to process and visualize each image\n",
    "keypoints_list = []\n",
    "descriptors_list = []\n",
    "\n",
    "for file_name in file_names[:2000]:\n",
    "    \n",
    "    image_path = 'images/images/' + file_name\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create(nfeatures=1000)\n",
    "\n",
    "    # Detect keypoints and descriptors\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "\n",
    "    # Save results\n",
    "    keypoints_list.append(keypoints)\n",
    "    descriptors_list.append(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2000 descriptors (1 for each image) - 6 min\n",
    "# Combine all descriptors into a single array\n",
    "all_descriptors = [desc for desc in descriptors_list if desc is not None]  # Filter out None\n",
    "all_descriptors = np.vstack(all_descriptors)\n",
    "\n",
    "# Perform clustering (e.g., K-Means)\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, \n",
    "                random_state=42)\n",
    "labels = kmeans.fit_predict(all_descriptors)\n",
    "\n",
    "# Map labels back to images\n",
    "descriptor_counts = [len(desc) if desc is not None else 0 for desc in descriptors_list]\n",
    "image_labels = []\n",
    "start = 0\n",
    "\n",
    "for count in descriptor_counts:\n",
    "    image_labels.append(labels[start:start + count])\n",
    "    start += count\n",
    "\n",
    "# # Example: print cluster assignments for each image\n",
    "# for img_name, lbl in zip(file_names[:5], image_labels[:5]):\n",
    "#     print(f\"Image: {img_name}, Clusters: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save descriptors and labels\n",
    "# with open('descriptors.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_descriptors, f)\n",
    "\n",
    "# with open('image_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(image_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustrating different clusters of descriptors of one image\n",
    "Have no idea what it gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a color map for clusters\n",
    "colors = np.random.randint(0, 255, size=(20, 3))\n",
    "\n",
    "\n",
    "# Visualize keypoints with cluster assignments\n",
    "i=123\n",
    "image_path = 'images/images/' + file_names[i]\n",
    "image = cv2.imread(image_path)\n",
    "keypoints = keypoints_list[i]\n",
    "labels = image_labels[i]\n",
    "\n",
    "# Draw keypoints with different colors based on cluster labels\n",
    "img_with_clusters = image.copy()\n",
    "for kp, label in zip(keypoints, labels):\n",
    "    color = colors[label].tolist()\n",
    "    x, y = int(kp.pt[0]), int(kp.pt[1])  # Keypoint coordinates\n",
    "    cv2.circle(img_with_clusters, (x, y), radius=3, color=color, thickness=-1)\n",
    "\n",
    "# Convert BGR to RGB for visualization in matplotlib\n",
    "img_with_clusters_rgb = cv2.cvtColor(img_with_clusters, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot the image with clustered keypoints\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img_with_clusters_rgb)\n",
    "plt.axis('off')\n",
    "plt.title('Keypoints Colored by Cluster Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering on amount of descriptors of each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count amount of each cluster among descriptors of each picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "image_vectors = []\n",
    "\n",
    "for i,descriptors in enumerate(descriptors_list):\n",
    "    \n",
    "    # Get cluster labels for the current image's descriptors\n",
    "    descriptors_labels = image_labels[i]\n",
    "    \n",
    "    # Count occurrences of each cluster in the current image\n",
    "    cluster_counts = Counter(descriptors_labels)\n",
    "    \n",
    "    # Convert to a fixed-size vector (bag of visual words representation)\n",
    "    feature_vector = np.zeros(20, dtype=int)\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        feature_vector[cluster_id] = count\n",
    "    \n",
    "    # Store the feature vector\n",
    "    image_vectors.append(feature_vector)\n",
    "\n",
    "# Convert list of feature vectors to a NumPy array for further processing\n",
    "image_vectors = np.array(image_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster descriptors histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster image_vectors\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(image_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holds the cluster id and the images { id: [images] }\n",
    "groups = {}\n",
    "for file, cluster in zip(file_names, kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "\n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25))\n",
    "    # gets the list of filenames for a cluster\n",
    "    files = groups[cluster]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(files) > 30:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 30\")\n",
    "        files = files[:29]\n",
    "    # plot each image in the cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10,10,index+1);\n",
    "        img = load_img('images/images/'+file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    print(f'Cluster {i}')\n",
    "    view_cluster(i)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG + KMeans\n",
    "https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file, model):\n",
    "    # load the image as a 224x224 array\n",
    "    img = load_img('images/images/'+file, \n",
    "                   target_size=(224,224))\n",
    "    # convert from 'PIL.Image.Image' to numpy array\n",
    "    img = np.array(img) \n",
    "\n",
    "    # <===== Maybe here B&W pictures has to be convert to GRB properly =====>\n",
    "\n",
    "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3)\n",
    "    # prepare image for model\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # get the feature vector\n",
    "    features = model.predict(imgx, use_multiprocessing=True)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\evaqw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\evaqw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "\n",
    "# 500 files - 4 min\n",
    "# 2000 - 15 min\n",
    "\n",
    "# loop through each image in the dataset\n",
    "for image in file_names[:2000]:\n",
    "    # try to extract the features and update the dictionary\n",
    "    try:\n",
    "        feat = extract_features(image, model)\n",
    "        data[image] = feat\n",
    "    # if something fails, save the extracted features as a pickle file (optional)\n",
    "    except:\n",
    "        print('failed: ', image)\n",
    "        continue\n",
    " \n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "\n",
    "# reshape so that there are 210 samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save this array: feat\n",
    "# np.save('feat.npy', feat)\n",
    "# # open it\n",
    "# feat_opened = np.load('feat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, random_state=42)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evaqw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=20, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=20, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=20,\n",
    "                random_state=42)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holds the cluster id and the images { id: [images] }\n",
    "groups = {}\n",
    "for file, cluster in zip(filenames, kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that lets you view a cluster (based on identifier)        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25))\n",
    "    # gets the list of filenames for a cluster\n",
    "    files = groups[cluster]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(files) > 30:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 30\")\n",
    "        files = files[:29]\n",
    "    # plot each image in the cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10,10,index+1);\n",
    "        img = load_img('images/images/'+file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    print(f'Cluster {i}')\n",
    "    view_cluster(i)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNET feature extraction and KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet model\n",
    "model = models.resnet101(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor)\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def cluster_images(features_list, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(features_list)\n",
    "    return kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the amount of images to process\n",
    "AMOUNT_OF_IMAGES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each image\n",
    "features_list = []\n",
    "\n",
    "for file_name in file_names[:AMOUNT_OF_IMAGES]:\n",
    "    try:\n",
    "        image_tensor = load_image(f'{folder_path}/{file_name}')\n",
    "        features = extract_features(image_tensor)\n",
    "        features_list.append(features)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {folder_path}/{file_name}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features list saved to emails_by_Prof_Bell\\features_list.pkl\n",
      "Loaded 1877 features.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# # Folder path to save the file\n",
    "# save_folder = 'emails_by_Prof_Bell'  # Replace with the folder where you want to save the file\n",
    "# save_file = 'features_list.pkl'  # The name of the file\n",
    "\n",
    "# # Ensure the folder exists\n",
    "# if not os.path.exists(save_folder):\n",
    "#     os.makedirs(save_folder)\n",
    "\n",
    "# # Full path to save the file\n",
    "# save_path = os.path.join(save_folder, save_file)\n",
    "\n",
    "# # Save the features_list using pickle\n",
    "# with open(save_path, 'wb') as f:\n",
    "#     pickle.dump(features_list, f)\n",
    "\n",
    "# print(f'Features list saved to {save_path}')\n",
    "\n",
    "# Load the features_list from the saved file\n",
    "with open(save_path, 'rb') as f:\n",
    "    loaded_features_list = pickle.load(f)\n",
    "\n",
    "print(f'Loaded {len(loaded_features_list)} features.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evaqw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "features_array = np.array(features_list)\n",
    "num_clusters = 20\n",
    "clusters = cluster_images(features_array, num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    m, n = 6, 7\n",
    "    N = m*n\n",
    "    fig, axes = plt.subplots(m, n, figsize=(m*2, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    idx = -1\n",
    "    # for idx, image_name in enumerate([file_names[i] for i in random_indexes]):\n",
    "    for image_name, label in zip(file_names[:AMOUNT_OF_IMAGES], clusters):\n",
    "        \n",
    "        if label != k:\n",
    "            continue\n",
    "        \n",
    "        if idx == N-1:\n",
    "            break\n",
    "        else:\n",
    "            idx += 1\n",
    "        \n",
    "        # Load the image\n",
    "        image_path = f'{folder_path}/{image_name}'\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "        \n",
    "            # Preprocess the image (handle grayscale to RGB conversion)\n",
    "            image_tensor = preprocess_image(image)\n",
    "\n",
    "            # Plot the image in the corresponding subplot\n",
    "            img_tensor = image_tensor.squeeze(0).permute(1, 2, 0)  # Remove batch dimension and rearrange to (H, W, C)\n",
    "            img_tensor = img_tensor * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])  # Un-normalize\n",
    "            img_tensor = torch.clamp(img_tensor, 0, 1)  # Clamp to [0,1]\n",
    "\n",
    "            # Convert to numpy array for plottAing\n",
    "            img_np = img_tensor.numpy()\n",
    "            \n",
    "            axes[idx].imshow(img_np)\n",
    "            axes[idx].axis('off')  # Hide axis\n",
    "            axes[idx].set_title(label, fontsize=8)  # Set the title as the predicted label\n",
    "        except Exception as e:  \n",
    "            print(f'Error processing {image_name}: {e}')\n",
    "\n",
    "    # Hide any remaining subplots if we have fewer images than subplots\n",
    "    for ax in axes[len(file_names[:N]):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Here you will receive around 30 samples of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# reduced_features = pca.fit_transform(features_array)\n",
    "tsne = TSNE(n_components=2)\n",
    "reduced_features = tsne.fit_transform(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imscatter(x, y, image_paths, ax=None, zoom=0.1):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    pictures = []\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        try:\n",
    "            img = plt.imread(folder_path+'/'+image_path)\n",
    "            if len(img.shape) == 2:  # grayscale\n",
    "                im = OffsetImage(img, zoom=zoom, cmap='gray')\n",
    "            else:  # color\n",
    "                im = OffsetImage(img, zoom=zoom)\n",
    "            ab = AnnotationBbox(im, (x[i], y[i]), xycoords='data', frameon=False)\n",
    "            pictures.append(ax.add_artist(ab))\n",
    "            ax.text(x[i], y[i] + 1.5, clusters[i], color='black', fontsize=9, ha='center')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {folder_path+'/'+image_path}: {e}\")\n",
    "    return pictures\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 20))\n",
    "x_vals = reduced_features[:, 0]\n",
    "y_vals = reduced_features[:, 1]\n",
    "\n",
    "IMAGES_TO_PLOT=1500\n",
    "imscatter(x_vals[:IMAGES_TO_PLOT], y_vals[:IMAGES_TO_PLOT], file_names[:IMAGES_TO_PLOT], ax=ax, zoom=0.025)\n",
    "# imscatter(x_vals, y_vals, file_names[:AMOUNT_OF_IMAGES], ax=ax, zoom=0.025)\n",
    "\n",
    "ax.set_title('Images clustering')\n",
    "plt.xlim(min(x_vals)+10, max(x_vals)+10)\n",
    "plt.ylim(min(y_vals)+10, max(y_vals)+10)\n",
    "plt.show()\n",
    "\n",
    "# Here you will receive a scatter plot with images as dots and their clusters labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNET classification - no ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the image transformations\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "#                          std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     # If the image is grayscale, convert it to RGB by duplicating the single channel\n",
    "#     if image.mode != 'RGB':\n",
    "#         image = image.convert('RGB')\n",
    "#     return preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "# # Visualize the image using matplotlib\n",
    "# def imshow(img_tensor):\n",
    "    \n",
    "#     img_tensor = img_tensor.squeeze(0)  # remove the batch dimension\n",
    "#     img_tensor = img_tensor.permute(1, 2, 0)  # change dimensions to (H, W, C)\n",
    "#     img_tensor = img_tensor * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])  # un-normalize\n",
    "#     img_tensor = torch.clamp(img_tensor, 0, 1)  # clamp to [0,1]\n",
    "    \n",
    "#     plt.imshow(img_tensor)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Load the pre-trained ResNet model\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# # # Load ImageNet class labels\n",
    "# # with open(\"imagenet-classes.txt\") as f:\n",
    "# #     labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(5, 7, figsize=(15, 10))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# random_indexes = random.sample(range(len(file_names)), 35)\n",
    "# selected_file_names = [file_names[i] for i in random_indexes]\n",
    "\n",
    "# for idx, image_name in enumerate([file_names[i] for i in random_indexes]):\n",
    "    \n",
    "#     # Load the image\n",
    "#     image_path = f'{folder_path}/{image_name}'\n",
    "#     image = Image.open(image_path)\n",
    "    \n",
    "#     # Preprocess the image (handle grayscale to RGB conversion)\n",
    "#     image_tensor = preprocess_image(image)\n",
    "\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         output = model(image_tensor)\n",
    "\n",
    "#     # Convert output probabilities to predicted class\n",
    "#     _, predicted_class = output.max(1)\n",
    "\n",
    "#     # Plot the image in the corresponding subplot\n",
    "#     img_tensor = image_tensor.squeeze(0).permute(1, 2, 0)  # Remove batch dimension and rearrange to (H, W, C)\n",
    "#     img_tensor = img_tensor * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])  # Un-normalize\n",
    "#     img_tensor = torch.clamp(img_tensor, 0, 1)  # Clamp to [0,1]\n",
    "\n",
    "#     # Convert to numpy array for plottAing\n",
    "#     img_np = img_tensor.numpy()\n",
    "\n",
    "#     axes[idx].imshow(img_np)\n",
    "#     axes[idx].axis('off')  # Hide axis\n",
    "#     axes[idx].set_title(labels[predicted_class.item()], fontsize=8)  # Set the title as the predicted label\n",
    "\n",
    "# # Hide any remaining subplots if we have fewer images than subplots\n",
    "# for ax in axes[len(file_names[:35]):]:\n",
    "#     ax.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TURTLE clusterisation\n",
    "The code is built with the following libraries\n",
    "\n",
    "PyTorch - 2.2.1  \n",
    "torchvision - 0.17.1  \n",
    "numpy  \n",
    "scipy  \n",
    "scikit-learn  \n",
    "clip  \n",
    "tqdm  \n",
    "cuml - 24.02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "wget https://brbiclab.epfl.ch/wp-content/uploads/2024/06/turtle_tasks.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ЌҐ г¤ Ґвбп ­ ©вЁ гЄ § ­­л© д ©«.\n"
     ]
    }
   ],
   "source": [
    "# pip create -n rapids-24.08 -c rapidsai -c conda-forge -c nvidia rapids=24.08 python=3.11 'cuda-version>=12.0,<=12.5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}